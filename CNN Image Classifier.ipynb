{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Lab 1 - Creating a CNN Classifier\n\nIn this lab, you will use your choice of PyTorch or Keras to create a convolutional neural network (CNN) to classify the vehicle images you prepared in the previous lab.\n\n> **Important**: If you did not complete Lab 1, use the sample solution notebook for that lab to prepare the training images you will need in this lab!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Set up a Deep Learning Framework\nYou can choose to build your CNN using **PyTorch** or **Keras**. You'll need to install the framework you want to use, and then import it (and any of its libraries you need to use):\n\n> **Hints**:\n> - Use the `pip` utility to install the framework - you can run shell commands in a notebook by prefixing the code with a **!** character"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# !pip install https://download.pytorch.org/whl/cpu/torch-1.0.1.post2-cp36-cp36m-linux_x86_64.whl\n# !pip install torchvision\n\n# Import PyTorch libraries\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nprint(\"Libraries imported - ready to use PyTorch\", torch.__version__)\n\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nfrom PIL import Image\nimport os",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Libraries imported - ready to use PyTorch 1.3.1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n\n\n\n\n\n\n\n## Prepare the Data\n\nIn the following cell, write code to load the training data and split it into a training set and a validation set.\n\n>**Hints**:\n> - If you are using PyTorch, use **ImageDataGenerator** objects to load the data. If you are using Keras, use **DataLoader** objects.\n> - Normalize the pixel values in the range 0-255 as you load them.\n> - Split the data into 70% for training, 30% for validation.\n> - Create a list of class names that corresponds to the class labels in your dataset."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n\n# The images are in a folder named 'shapes/training'\n#training_folder_name = \"../library/data/classification/training\"\ntraining_folder_name = \"../data/classification/training2\"\n# The folder contains a subfolder for each class of shape\nclasses = sorted(os.listdir(training_folder_name))\nprint(classes)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['automobile', 'plane', 'train']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# from PIL import Image, ImageOps\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# catagories = os.listdir(training_folder_name)\n\n# tracker = 1\n# for root, dirs, files in os.walk(training_folder_name):\n#     try:\n#         if os.path.split(root)[1] == 'training':\n#             pass\n#         else:\n#             sFiles = sorted(files)\n#             for i in sFiles:\n#                 #print(i)\n#                 #print(catagories[tracker-1])\n#                 image = Image.open(os.path.join(root, i))\n#                 size  = image.size\n#                 if size != (128, 128):\n#                     print(i)\n#                     print(catagories[tracker-1])\n#                     print(image.size)\n#             tracker += 1\n#     except IndexError:\n#         pass\n# plt.show()",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def find_train_test_len(data, train_perc, test_perc):\n    assert train_perc + test_perc == 1., \"Percentages must add to 100\"\n    len_data = len(data)\n    train_len = int(len_data*train_perc)\n    test_len = len_data - train_len\n    return train_len, test_len\n    \n\n\ndef Data_Loadup(Data_Path):\n    #create data transformer (Convert to tensors and normalize data)\n    transformer = transforms.Compose([transforms.RandomHorizontalFlip(0.5),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])])\n    \n    #load & transform data\n    DataSet = torchvision.datasets.ImageFolder(root = Data_Path, transform = transformer)\n    \n    #Determine lengths of training and testing sets\n    train_len, test_len = find_train_test_len(DataSet, 0.7, 0.3)\n    \n    #Split data into training and testing sets\n    train_set, test_set = torch.utils.data.random_split( DataSet, [train_len, test_len] )\n    \n    #Set up the Data Loaders (feeds loaded data to CNN)\n    train_loader = torch.utils.data.DataLoader(train_set, batch_size=18, num_workers=0, shuffle=False)\n    test_loader = torch.utils.data.DataLoader(test_set, batch_size=18, num_workers=0, shuffle=False)\n    \n    return train_loader, test_loader\n\n# Get the iterative dataloaders for test and training data\ntrain_loader, test_loader = Data_Loadup(training_folder_name)\nbatch_size = train_loader.batch_size\nprint(\"Data loaders ready to read\", training_folder_name)\n",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Data loaders ready to read ../data/classification/training2\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Define the CNN\nDefine the layers in your CNN:\n\n> **Hints**:\n> - Use *convolution* layers to extract feature based on filter kernels.\n> - Use pooling layers to downsample the extracted features.\n> - To improve your model, experiment with adding layers (making the network deeper) and adding filters (making the network broader)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class my_CNN(nn.Module):\n    \n    #Define constructor\n    def __init__(self, num_classes=3):\n        #Ensures proper MRO\n        super(my_CNN, self).__init__()\n        \n        #Define Layers\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=24, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, stride=1, padding=1)\n        #self.conv4 = nn.Conv2d(in_channels=48, out_channels=144, kernel_size=3, stride=1, padding=1)\n        \n        self.drop  = nn.Dropout2d(p=0.15)\n        self.dropfc= nn.Dropout2d(p=0.5)\n        self.pool  = nn.MaxPool2d(kernel_size=2)\n        self.fc    = nn.Linear(in_features=16*16*48, out_features=num_classes)\n        #self.fc    = nn.Linear(in_features=16*16*48, out_features=num_classes)\n        \n    #Define Forward Function (How the CNN goes through the layers and data)\n    def forward(self, x): \n        #Layer 1 goes: 12 filters > 1 pooling > RELU\n        x = F.relu(self.pool(self.conv1(x)))\n        x = self.drop(x)\n        x = F.relu(self.pool(self.conv3(x)))\n        x = self.drop(x)\n        x = F.relu(self.conv3(x))\n        x = self.drop(x)\n        \n        x = F.relu(self.conv3(x))\n        x = self.drop(x)\n        #Layer 2 goes: 24 filters > 1 pooling > RELU\n        #x = F.relu(self.pool(self.conv2(x)))\n        \n        #Layer 3 goes: 48 filters > 1 pooling > RELU\n        x = F.relu(self.pool(self.conv2(x)))\n        x = self.drop(x)\n\n        #Flatten Output\n        #x = x.view(-1, 32*32*24)\n        x = x.view(-1, 16*16*48)\n        \n        x = self.dropfc(self.fc(x))\n        #Return class probabilities\n        return torch.log_softmax(x, dim=1)\n    \n\n# #Set the device to which the device is allocated\n# device = 'cpu'\n# if (torch.cuda.is_available()):\n#     device = 'cuda'\n    \n# #Create model Instance\n# model = my_CNN(num_classes=3).to(device)\n# print(model)\n\n            \n        ",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Import Pre-Built Model (ResNet34)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = torchvision.models.resnet34(pretrained=True)\nfor param in model.parameters():\n    param.requires_grad = False\n    \nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, len(classes))\n\nprint(model)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=3, bias=True)\n)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train the Model\n\nTrain the model using the training dataset, and validating after each epoch with the validation dataset.\n\n> **Hints**:\n> - Choose an appropriate optimizer and learning rate - Refer to the [PyTorch](https://pytorch.org/docs/stable/optim.html#algorithms) or [Keras](https://keras.io/optimizers/) framework documentation as you experiment with this.\n> - Run the training process over a reasonable number of epochs - start with 15 and increase / decrease based on your observations of validation loss.\n> - Track both training and validation loss so you can observe training progress and plot the loss history after training."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def train(model, device, train_loader, optimizer, epoch):\n    model.train\n    train_loss = 0 \n    print(\"Epoch: \", epoch)\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_criteria(output, target)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        print('\\tTraining batch {} Loss:{:.6f}'.format(batch_idx+1, loss.item()))\n        \n    avg_loss = train_loss / (batch_idx + 1)\n    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n    return avg_loss\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        batch_count = 0\n        for data, target in test_loader:\n            batch_count += 1\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += loss_criteria(output, target).item()\n            _, predicted = torch.max(output.data, 1)\n            correct += torch.sum(target == predicted).item()\n        avg_loss = test_loss / batch_count\n        print('Validation Set: Average Loss:{:.6f}, Accuracy:{}/{} ({:.0f}%)\\n'.format(\n            avg_loss, correct, len(test_loader.dataset), \n            100. * correct / len(test_loader.dataset)))\n        return avg_loss\n    \n    \n#Set the device to which the device is allocated\ndevice = 'cpu'\nif (torch.cuda.is_available()):\n    device = 'cuda'    \n\n    \noptimizer = optim.Adam(model.parameters(), lr = 0.001)\nloss_criteria = nn.CrossEntropyLoss()\nepoch_nums = []\ntraining_loss = []\nvalidation_loss = []\n\nepochs = 10\nprint('Training on ', device )\nfor epoch in range(1, epochs+1):\n    train_loss = train(model, device, train_loader, optimizer, epoch)\n    test_loss  = test(model, device, test_loader)\n    epoch_nums.append(epoch)\n    training_loss.append(train_loss)\n    validation_loss.append(test_loss)\n    \n        ",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training on  cpu\nEpoch:  1\n\tTraining batch 1 Loss:0.793905\n\tTraining batch 2 Loss:0.809396\n\tTraining batch 3 Loss:0.721905\n\tTraining batch 4 Loss:0.748564\n\tTraining batch 5 Loss:0.771620\n\tTraining batch 6 Loss:0.671755\n\tTraining batch 7 Loss:0.542736\n\tTraining batch 8 Loss:0.830158\n\tTraining batch 9 Loss:0.459240\n\tTraining batch 10 Loss:0.564194\nTraining set: Average loss: 0.691347\nValidation Set: Average Loss:0.520061, Accuracy:61/72 (85%)\n\nEpoch:  2\n\tTraining batch 1 Loss:0.368471\n\tTraining batch 2 Loss:0.391744\n\tTraining batch 3 Loss:0.354563\n\tTraining batch 4 Loss:0.470679\n\tTraining batch 5 Loss:0.295455\n\tTraining batch 6 Loss:0.408218\n\tTraining batch 7 Loss:0.303657\n\tTraining batch 8 Loss:0.505158\n\tTraining batch 9 Loss:0.218339\n\tTraining batch 10 Loss:0.290617\nTraining set: Average loss: 0.360690\nValidation Set: Average Loss:0.322633, Accuracy:68/72 (94%)\n\nEpoch:  3\n\tTraining batch 1 Loss:0.254298\n\tTraining batch 2 Loss:0.167795\n\tTraining batch 3 Loss:0.225639\n\tTraining batch 4 Loss:0.236766\n\tTraining batch 5 Loss:0.201749\n\tTraining batch 6 Loss:0.272129\n\tTraining batch 7 Loss:0.235684\n\tTraining batch 8 Loss:0.299641\n\tTraining batch 9 Loss:0.189584\n\tTraining batch 10 Loss:0.170069\nTraining set: Average loss: 0.225336\nValidation Set: Average Loss:0.245346, Accuracy:70/72 (97%)\n\nEpoch:  4\n\tTraining batch 1 Loss:0.166386\n\tTraining batch 2 Loss:0.111037\n\tTraining batch 3 Loss:0.183327\n\tTraining batch 4 Loss:0.215599\n\tTraining batch 5 Loss:0.172149\n\tTraining batch 6 Loss:0.200599\n\tTraining batch 7 Loss:0.173179\n\tTraining batch 8 Loss:0.199784\n\tTraining batch 9 Loss:0.108672\n\tTraining batch 10 Loss:0.123711\nTraining set: Average loss: 0.165444\nValidation Set: Average Loss:0.221022, Accuracy:71/72 (99%)\n\nEpoch:  5\n\tTraining batch 1 Loss:0.181808\n\tTraining batch 2 Loss:0.091313\n\tTraining batch 3 Loss:0.115687\n\tTraining batch 4 Loss:0.172402\n\tTraining batch 5 Loss:0.098866\n\tTraining batch 6 Loss:0.201137\n\tTraining batch 7 Loss:0.114752\n\tTraining batch 8 Loss:0.152691\n\tTraining batch 9 Loss:0.093892\n\tTraining batch 10 Loss:0.088190\nTraining set: Average loss: 0.131074\nValidation Set: Average Loss:0.178802, Accuracy:71/72 (99%)\n\nEpoch:  6\n\tTraining batch 1 Loss:0.094094\n\tTraining batch 2 Loss:0.082615\n\tTraining batch 3 Loss:0.080384\n\tTraining batch 4 Loss:0.120206\n\tTraining batch 5 Loss:0.077283\n\tTraining batch 6 Loss:0.138699\n\tTraining batch 7 Loss:0.140775\n\tTraining batch 8 Loss:0.142213\n\tTraining batch 9 Loss:0.111389\n\tTraining batch 10 Loss:0.069911\nTraining set: Average loss: 0.105757\nValidation Set: Average Loss:0.180781, Accuracy:70/72 (97%)\n\nEpoch:  7\n\tTraining batch 1 Loss:0.075514\n\tTraining batch 2 Loss:0.052678\n\tTraining batch 3 Loss:0.079691\n\tTraining batch 4 Loss:0.093075\n\tTraining batch 5 Loss:0.064926\n\tTraining batch 6 Loss:0.152602\n\tTraining batch 7 Loss:0.073311\n\tTraining batch 8 Loss:0.143994\n\tTraining batch 9 Loss:0.048207\n\tTraining batch 10 Loss:0.055920\nTraining set: Average loss: 0.083992\nValidation Set: Average Loss:0.148637, Accuracy:71/72 (99%)\n\nEpoch:  8\n\tTraining batch 1 Loss:0.084760\n\tTraining batch 2 Loss:0.044075\n\tTraining batch 3 Loss:0.084976\n\tTraining batch 4 Loss:0.078070\n\tTraining batch 5 Loss:0.051766\n\tTraining batch 6 Loss:0.147217\n\tTraining batch 7 Loss:0.054087\n\tTraining batch 8 Loss:0.083616\n\tTraining batch 9 Loss:0.064661\n\tTraining batch 10 Loss:0.050649\nTraining set: Average loss: 0.074388\nValidation Set: Average Loss:0.137271, Accuracy:71/72 (99%)\n\nEpoch:  9\n\tTraining batch 1 Loss:0.071543\n\tTraining batch 2 Loss:0.030546\n\tTraining batch 3 Loss:0.036157\n\tTraining batch 4 Loss:0.070815\n\tTraining batch 5 Loss:0.053189\n\tTraining batch 6 Loss:0.139221\n\tTraining batch 7 Loss:0.089163\n\tTraining batch 8 Loss:0.084514\n\tTraining batch 9 Loss:0.067479\n\tTraining batch 10 Loss:0.047628\nTraining set: Average loss: 0.069025\nValidation Set: Average Loss:0.128011, Accuracy:69/72 (96%)\n\nEpoch:  10\n\tTraining batch 1 Loss:0.091979\n\tTraining batch 2 Loss:0.040212\n\tTraining batch 3 Loss:0.040664\n\tTraining batch 4 Loss:0.063325\n\tTraining batch 5 Loss:0.093847\n\tTraining batch 6 Loss:0.078641\n\tTraining batch 7 Loss:0.054741\n\tTraining batch 8 Loss:0.061806\n\tTraining batch 9 Loss:0.062297\n\tTraining batch 10 Loss:0.038568\nTraining set: Average loss: 0.062608\nValidation Set: Average Loss:0.130727, Accuracy:70/72 (97%)\n\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## View Loss History\n\nUse matplotlib to plot the training and validation loss history.\n\n> **Hints**:\n> - Experiment with adding model layers and filters, changing activation functions,, or using a different optimizer or learning rate to achieve a lower loss - you should restart the kernel between each attempt!\n> - If the training loss continues to drop, but the validation loss doesn't - the model is *overfitting*. We'll explore options to address this later in the course."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n\nplt.plot(epoch_nums, training_loss)\nplt.plot(epoch_nums, validation_loss)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## View Model Performance\n\nPlot a confusion matrix to observe how the model performs:\n\n> **Hints**:\n> - Use some or all of the validation data to generate metrics for the confusion matrix.\n> - Use the **sklearn.metrics.confusion_matrix** object to create the confusion matrix."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n\n#set-up for evaluation\ntruelabels   = []\npredicitions = []\nmodel.eval()\n\nfor data, target in test_loader:\n    for label in target.data.numpy():\n        truelabels.append(label)\n    for predicition in model(data).data.numpy().argmax(1):\n        predicitions.append(predicition)\n        \ncm = confusion_matrix(truelabels, predicitions)\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.colorbar()\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=85)\nplt.yticks(tick_marks, classes)\nplt.xlabel(\"Predicted Shape\")\nplt.ylabel(\"True Shape\")\nplt.show()",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 2 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Save the Model Weights\n\nAfter you've trained the model, save the model weights so you can reuse it later.\n\n> **Hints**:\n> - Save the weights, not the entire model.\n> - See [Keras documentation](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) for information about saving Keras model weights, or [PyTorch documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for information about saving PyTorch weights. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_file = 'vehicle_classifier_resnet34.pt'\ntorch.save(model.state_dict(), model_file)\nprint('Saved')\ndel model\n",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Saved\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Use the Model to Classify New Images\n\nNow you can use your model to infer the class of new images.\n\n> **Hints**:\n- Use the images in the **../data/classification/test** folder to test your model.\n- Remember to prepare your new imagfes the same way you prepared the training images - they must be the same size and have the same transformations applied to them.\n- Consider creating functions to encapsulate image preparation class prediction."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def predict_image(classifier, image_array):\n    classifier.eval()\n    class_names = ['automobile', 'train', 'plane']\n    transformation = transforms.Compose([transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])])\n    image_tensor = torch.stack([transformation(image).float() for image in image_array])\n    \n    \n    input_features = image_tensor\n    predictions = classifier(input_features)\n    predicted_classes = []\n    \n    for prediction in predictions.data.numpy():\n        class_idx = np.argmax(prediction)\n        predicted_classes.append(class_names[class_idx])\n        \n    return np.array(predicted_classes)\n\nmodel_file = 'vehicle_classifier_resnet34.pt'\n\n\nmodel = torchvision.models.resnet34()\nmodel.load_state_dict(torch.load(model_file))\n\ntest_folder = \"../data/classification/training/train\"\ntest_images = os.listdir(test_folder)\n\nimage_arrays = []\n\n\nfor file in test_images:\n    img = np.array(Image.open(os.path.join(test_folder, file)))\n    image_arrays.append(img)\n    \n\npredictions = predict_image(model, np.array(image_arrays))\n\nfig = plt.figure(figsize=(200, 200))\nfor idx, prediction in enumerate(predictions):\n    if idx < 5:\n        a = fig.add_subplot(1, len(predictions), idx+1)\n        imgplot = plt.imshow(image_arrays[idx])\n        a.set_title(prediction)\n        ",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([3, 512]) from checkpoint, the shape in current model is torch.Size([1000, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([1000]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-75f1665d1da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtest_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/classification/training/train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([3, 512]) from checkpoint, the shape in current model is torch.Size([1000, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([1000])."
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}